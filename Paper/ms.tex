\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{dsfont}
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}

\newcommand{\xx}{\mathbf{x}}	% The unknown parameters
\newcommand{\data}{\mathbf{D}}  % The data
\newcommand{\dx}{d^N\mathbf{x}} % Volume element in parameter space
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\parindent=0cm

\title{Nested sampling with multiple scalars}
\author{Brendon J. Brewer and Ewan Cameron}

\begin{document}
\maketitle

\abstract{The abstract goes here.}

\section{Introduction}
Nested Sampling \citep{skilling} is an effective and popular
Monte Carlo algorithm for Bayesian computation and
statistical mechanics
\citep{2009arXiv0906.3544P, 2014PhRvE..89b2302P, 2015arXiv150303404B}.
In a Bayesian inference problem with unknown parameters denoted collectively
by a vector $\xx$, the
posterior distribution for the parameters given data $\data$ (and prior
information $I$) is:
\begin{eqnarray}
p(\xx | \data, I) &=&
\frac{p(\xx | I)p(\data | \xx, I)}{p(\data | I)}\\
&=& \frac{\pi(\xx)L(\xx)}{Z}
\end{eqnarray}
where $\pi(\xx)$ is the prior distribution, $L(\xx)$ is the likelihood
function, and $Z$ is the normalising constant, known as the
``marginal likelihood'' or the ``evidence'':
\begin{eqnarray}
Z &=& \int \pi(\xx) L(\xx) \, \dx.
\end{eqnarray}

Nested Sampling begins by drawing particles from the
prior $\pi(\xx)$ and successively imposing constraints on the value of
the likelihood $L(\xx)$ that compress the prior mass by a
factor that is approximately known.
This enables the calculation of the marginal likelihood
and properties of the posterior, or any other distribution that is intermediate
between the prior and the posterior. For example, we might be interested in
a ``power posterior'' where the likelihood is raised to a power $\beta$:
\begin{eqnarray}
p(\xx; \beta) &=& \frac{\pi(\xx)L(\xx)^\beta}{Z(\beta)}
\end{eqnarray}
The normalisation and posterior samples from this distribution can be obtained
from the original Nested Sampling run. An example application, in the case of
a Bayesian model with a ``gaussian noise'' assumption in the likelihood,
computing $p(\xx; \beta)$ for $\beta \neq 1$ allows us to explore what the
posterior distribution would have been if the noise variance had been greater.

Relative to alternative approaches such as annealing, Nested Sampling continues
to work when the problem contains a {\it phase transition}.

In some inference and
statistical mechanics problems, there are two or more scalar functions of
$\xx$ that are relevant. Suppose our prior is $\pi(\xx)$ as before, and
we obtain testable information that fixes the expected values of two scalar
functions of $\xx$, $S_1(\xx)$ and $S_2(\xx)$. It is well known that the
updated probability distribution that takes into account the constraints is
of the ``canonical'' form:

\begin{eqnarray}
p(\xx; \alpha_1, \alpha_2) &=& \frac{\pi(\xx)\exp\left[\alpha_1S_1(\xx)+\alpha_2S_2(\xx)\right]}
{Z(\alpha_1, \alpha_2)}
\end{eqnarray}

If we were only interested in a single canonical distribution, for example
with $\alpha_1 = 0.3$ and $\alpha_2 = 0.7$, we could estimate its normalising
constant and by running standard Nested Sampling with ``likelihood''
$L(\xx) = \exp\left[0.3S_1(\xx) + 0.7S_2(\xx)\right]$. However, usually we
are interested in a range of values for $\alpha_1$ and $\alpha_2$, and we
want to know the normalisation $Z(\alpha_1, \alpha_2)$, called the
partition function. This work describes
progress towards solving this class of problems while maintaining the benefits
of Nested Sampling, such as the ability to cope with first-order phase
transitions.

\section{Properties of Nested sampling}
We seek an algorithm for computing the partition function
$Z(\alpha_1, \alpha_2)$ for a range of values of the $\alpha$s, from a single
run. To be considered a variant of Nested Sampling, we require that the method
satisfies the following requirements:
\begin{itemize}
\item The algorithm should begin with $n$ points drawn from the prior $\pi(\xx)$.
\item The algorithm should seek to explore regions where the values of
$S_1(\xx)$ and $S_2(\xx)$ are greater than what would be expected from the
prior.
\item The algorithm should try to move through a sequence of probability
distributions whose enclosed prior masses are approximately known.
\item The algorithm should be invariant to monotonic transformations of
$S_1$ and $S_2$, i.e. it should only depend on rankings of $S_1$ and $S_2$
values and not the values themselves.
\end{itemize}


\section{Demonstration Example}
The prior is a uniform distribution over the $n$-dimensional unit hypercube:
\begin{eqnarray}
p(\xx) &=& 1
\end{eqnarray}
as long as $x_i \in [0, 1]$ for all $i$. The scalars are:
\begin{eqnarray}
S_1(\xx) &=& -\sum_{i=1}^n \left(x_i - 0.5\right)^2\\
S_2(\xx) &=& -\sum_{i=1}^n \sin^2\left(4\pi x_i\right)
\end{eqnarray}

For this section, we used $n=1000$ dimensions.

\section{Example: Potts model with some embellishments}
TBD

\begin{thebibliography}{99}

\bibitem[Baldock et al.(2015)]{2015arXiv150303404B} Baldock, R.~J.~N., 
P{\'a}rtay, L.~v.~B., Bart{\'o}k, A.~P., Payne, M.~C., Cs{\'a}nyi, G.\ 
2015.\ Determining pressure-temperature phase diagrams of materials.\ ArXiv 
e-prints arXiv:1503.03404. 

\bibitem[P{\'a}rtay et al.(2014)]{2014PhRvE..89b2302P} P{\'a}rtay, L.~B., 
Bart{\'o}k, A.~P., Cs{\'a}nyi, G.\ 2014.\ Nested sampling for materials: 
The case of hard spheres.\ Physical Review E 89, 022302. 

\bibitem[P{\'a}rtay et al.(2009)]{2009arXiv0906.3544P} P{\'a}rtay, L.~B., 
Bart{\'o}k, A.~P., Cs{\'a}nyi, G.\ 2009.\ Efficient sampling of atomic 
configurational spaces.\ ArXiv e-prints arXiv:0906.3544. 

\bibitem[\protect\citeauthoryear{Skilling}{2006}]{skilling} Skilling, J., 2006, Nested Sampling for General Bayesian Computation, Bayesian Analysis 4, pp. 833-860.

\end{thebibliography}


\end{document}

