\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{dsfont}
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}

\newcommand{\xx}{\mathbf{x}}	% The unknown parameters
\newcommand{\dx}{d^N\mathbf{x}} % Volume element in parameter space
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\parindent=0cm

\title{Nested sampling with multiple scalars}
\author{Brendon J. Brewer and Ewan Cameron}

\begin{document}
\maketitle

\abstract{The abstract goes here.}

\section{Introduction}
Nested Sampling \citep{skilling} is an effective and popular
Monte Carlo algorithm for Bayesian computation and statistical mechanics.
In a Bayesian inference problem with unknown parameters denoted collectively
by a vector $\mathbf{x}$, the
posterior distribution for the parameters is
\begin{eqnarray}
p(\mathbf{x} | {\rm data}) &=&
\frac{p(\mathbf{x})p({\rm data} | \mathbf{x})}{p(\rm data)}\\
&=& \frac{\pi(\xx)L(\xx)}{Z}
\end{eqnarray}
where $\pi(\xx)$ is the prior distribution, $L(\xx)$ is the likelihood
function, and $Z$ is the normalising constant, known as the
``marginal likelihood'' or the ``evidence'':
\begin{eqnarray}
Z &=& \int \pi(\xx) L(\xx) \, \dx.
\end{eqnarray}

Nested Sampling begins by drawing particles from the
prior $\pi(\xx)$ and successively imposing constraints on the value of
the likelihood $L(\xx)$ that compress the prior mass by a
factor that is approximately known.
This enables the calculation of the marginal likelihood
and properties of the posterior, or any other distribution that is intermediate
between the prior and the posterior.

Relative to alternative approaches such as annealing, Nested Sampling continues
to work when the problem contains a {\it phase transition}.

In some inference and
statistical mechanics problems, there are two or more scalar functions of
$\xx$ that are relevant. Suppose our prior is $\pi(\xx)$ as before, and
we obtain testable information that fixes the expected values of two scalar
functions of $\xx$, $S_1(\xx)$ and $S_2(\xx)$. It is well known that the
updated probability distribution that takes into account the constraints is
of the ``canonical'' form:

\begin{eqnarray}
p(\xx; \alpha_1, \alpha_2) &=& \frac{\pi(\xx)\exp\left[\alpha_1S_1(\xx)+\alpha_2S_2(\xx)\right]}
{Z(\alpha_1, \alpha_2)}
\end{eqnarray}

 For example, we might be interested in the
properties of the canonical distributions
$p(\mathbf{x} | \alpha, \beta) \propto \pi(\mathbf{x})\exp\left[\alpha f_1(\mathbf{x}) + \beta f_2(\mathbf{x})\right]$
for many different values of $\alpha$ and $\beta$, including the
normalisations or ``partition function'' $\mathcal{Z}(\alpha, \beta)$.
This work describes
progress towards solving this class of problems while maintaining the benefits
of Nested Sampling, such as the ability to cope with first-order phase
transitions.

If the prior is $\pi(\mathbf{x})$, then a family of canonical
probability distributions is given by
\begin{eqnarray}
p(\mathbf{x} | \lambda_1, ..., \lambda_n) = 
\frac{\pi(\mathbf{x})\exp(-\sum_{i=1}^n \lambda_i f_i(\mathbf{x}))}
{\mathcal{Z}(\lambda_1, ..., \lambda_n)}
\end{eqnarray}
This family of distributions arises from a MaxEnt update from the prior
$\pi(\mathbf{x})$ if the expected values of the functions
$\{f_i(\mathbf{x})\}$ are specified. In statistical mechanics it is useful to
quantify the properties of
$p(\mathbf{x} | \lambda_1, ..., \lambda_n)$ as a function of the
$\lambda$s.

\section{Nested sampling}


\section{Demonstration Example}
The prior is a uniform distribution over the $n$-dimensional unit hypercube:
\begin{eqnarray}
p(\mathbf{x}) &=& 1
\end{eqnarray}
as long as $x_i \in [0, 1]$ for all $i$. The scalars are:
\begin{eqnarray}
S_1(\mathbf{x}) &=& -\sum_{i=1}^n \left(x_i - 0.5\right)^2\\
S_2(\mathbf{x}) &=& -\sum_{i=1}^n \sin^2\left(4\pi x_i\right)
\end{eqnarray}

For this section, we used $n=1000$ dimensions.

\section{Example: Potts model with some embellishments}
TBD

\begin{thebibliography}{99}

\bibitem[\protect\citeauthoryear{Skilling}{2006}]{skilling} Skilling, J., 2006, Nested Sampling for General Bayesian Computation, Bayesian Analysis 4, pp. 833-860.

\end{thebibliography}


\end{document}

